{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHabgZAdLlTu"
      },
      "source": [
        "## **1. Setting up the API and dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luZ-bTqZW0Qh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"XXXXX\"\n",
        "os.environ['KAGGLE_KEY'] = \"XXXXX\"\n",
        "!kaggle datasets download -d yelp-dataset/yelp-dataset\n",
        "!mkdir reviews #creating a new directory for the dataset\n",
        "!unzip yelp-dataset.zip -d reviews #unzipping the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boDwWSFDLx_6"
      },
      "source": [
        "## **2. Pre-Processing Methods**\n",
        "\n",
        "#### *2.1 Data Cleaning*\n",
        "*   Lowercase All Text\n",
        "*   Removing Punctuations\n",
        "*   Word Lemmatisation\n",
        "*   Removing Stop-words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG5V-gIXHbfg"
      },
      "outputs": [],
      "source": [
        "#!pip install datasketch\n",
        "#!pip install nltk\n",
        "\n",
        "import string\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datasketch import MinHash, MinHashLSH\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "def clean_data(text):\n",
        "    text = text.lower()\n",
        "    text = remove_punctuation(text)\n",
        "    split_text = text.split()\n",
        "    remove_and_lemma = [lemma.lemmatize(word) for word in split_text if word not in stop_words]\n",
        "    cleaned_text = ' '.join(remove_and_lemma)\n",
        "    return cleaned_text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *2.2 K-Shingling*"
      ],
      "metadata": {
        "id": "x4BLgScXd4wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def k_shingle(text):\n",
        "    words = text.split()\n",
        "    for i in range(len(words) - 1):\n",
        "        shingle = ' '.join([words[i], words[i+1]])\n",
        "        yield shingle"
      ],
      "metadata": {
        "id": "2BnVjGubd2Hm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBupAky0MTI-"
      },
      "source": [
        "## **3. Processing Reviews & Building MinHashLSH Index**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwYPeB57BnGQ"
      },
      "outputs": [],
      "source": [
        "chunk_size = 10000\n",
        "num_perm = 128 \n",
        "threshold = 0.75\n",
        "\n",
        "lsh = MinHashLSH(num_perm=num_perm, threshold=threshold) #initialise lsh\n",
        "\n",
        "processed_reviews = []\n",
        "num_reviews_processed = 0\n",
        "num_chunks = 60\n",
        "\n",
        "with tqdm(total=num_chunks, desc='Processing Chunks', unit='chunk') as pbar: #creating progress bar\n",
        "    # Iterate over the data chunks\n",
        "    for chunk_no, chunk in enumerate(pd.read_json(\"reviews/yelp_academic_dataset_review.json\", lines=True, chunksize=chunk_size)):\n",
        "        if chunk_no >= num_chunks:  #break the loop after desired number of chunks\n",
        "            break\n",
        "\n",
        "        chunk['pre_cleaned_text'] = chunk['text']  #add pre cleaned text column\n",
        "        chunk['text'] = chunk['text'].apply(clean_data)\n",
        "\n",
        "        processed_chunk = []\n",
        "        for ind, row in chunk.iterrows():  \n",
        "            shingles = list(k_shingle(row['text'])) #k-shingling\n",
        "            minhash = MinHash(num_perm=num_perm) \n",
        "\n",
        "            for shingle in shingles:\n",
        "                minhash.update(shingle.encode('utf-8'))\n",
        "\n",
        "            processed_chunk.append({\n",
        "                'review_id': row['review_id'],\n",
        "                'user_id': row['user_id'],\n",
        "                'business_id': row['business_id'],\n",
        "                'stars': row['stars'],\n",
        "                'pre_cleaned_text': row['pre_cleaned_text'],\n",
        "                'shingles': shingles,\n",
        "                'minhash': minhash})\n",
        "\n",
        "        if chunk_no < 1:\n",
        "            print(\"\\n\\nFirst 5 Processed Reviews:\\n\") #printing first 5 processed reviews\n",
        "\n",
        "            for i in range(5):\n",
        "                print(\"\\nReview\", i+1, \":\")\n",
        "                review = processed_chunk[i]\n",
        "                print(\"Review ID:\", review['review_id'])\n",
        "                print(\"User ID:\", review['user_id'])\n",
        "                print(\"Business ID:\", review['business_id'])\n",
        "                print(\"Stars:\", review['stars'])\n",
        "                print(\"Pre-cleaned Text:\", review['pre_cleaned_text'])\n",
        "                print(\"Shingles:\", review['shingles'])\n",
        "\n",
        "        for item in processed_chunk:\n",
        "            lsh.insert(item['review_id'], item['minhash']) #index minhash objects in lsh\n",
        "        \n",
        "        processed_reviews.extend(processed_chunk)\n",
        "        pbar.update(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaoNRvmxFpih"
      },
      "source": [
        "\n",
        "## **4. Finding Similar Pairs**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "similar_pairs = []\n",
        "for chunk_no, chunk in enumerate(pd.read_json(\"reviews/yelp_academic_dataset_review.json\", lines=True, chunksize=chunk_size)):\n",
        "    if chunk_no >= num_chunks:  #break the loop after the specified number of chunks\n",
        "        break\n",
        "\n",
        "    chunk['pre_cleaned_text'] = chunk['text']  #add pre clean text column\n",
        "    chunk['text'] = chunk['text'].apply(clean_data)\n",
        "\n",
        "    for ind, row in chunk.iterrows():  \n",
        "        shingles = list(k_shingle(row['text'])) #k-shingling\n",
        "        minhash = MinHash(num_perm=num_perm)\n",
        "        \n",
        "        for shingle in shingles:\n",
        "            minhash.update(shingle.encode('utf-8'))\n",
        "\n",
        "        similar_items = lsh.query(minhash) #lsh query for similar items\n",
        "\n",
        "        for item in similar_items: \n",
        "            if item != row['review_id']:  #remove self-matches\n",
        "                similar_pairs.append((row['review_id'], item)) #appending similar pairs"
      ],
      "metadata": {
        "id": "Oa60RthKvjYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Similar Pairs Dataframe**"
      ],
      "metadata": {
        "id": "PzMLfHTmWkFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_clean = pd.DataFrame(processed_reviews) \n",
        "\n",
        "similar_pairs_df = pd.DataFrame(similar_pairs, columns=['review_id_1', 'review_id_2']) #create df with similar pairs\n",
        "\n",
        "merged_df_1 = pd.merge(similar_pairs_df, data_clean[['review_id', #merge with original dataset using left join\n",
        "                                                     'user_id', \n",
        "                                                     'business_id', \n",
        "                                                     'stars', \n",
        "                                                     'pre_cleaned_text']],\n",
        "                       left_on='review_id_1', right_on='review_id', how='left')\n",
        "\n",
        "renamed_df_1 = merged_df_1.rename(columns={'user_id': 'user_id_1', #rename to prevent confusion\n",
        "                                           'business_id': 'business_id_1',\n",
        "                                           'stars': 'stars_1',\n",
        "                                           'pre_cleaned_text': 'pre_cleaned_text_1'})\n",
        "\n",
        "dropped_df_1 = renamed_df_1.drop(columns=['review_id']) #drop unnecessary columns\n",
        "\n",
        "merged_df_2 = pd.merge(dropped_df_1, data_clean[['review_id', #merge using second review id using left join\n",
        "                                                 'user_id', \n",
        "                                                 'business_id', \n",
        "                                                 'stars', \n",
        "                                                 'pre_cleaned_text']],\n",
        "                       left_on='review_id_2', right_on='review_id', how='left')\n",
        "\n",
        "renamed_df_2 = merged_df_2.rename(columns={'user_id': 'user_id_2', #rename to prevent confusion\n",
        "                                           'business_id': 'business_id_2',\n",
        "                                           'stars': 'stars_2',\n",
        "                                           'pre_cleaned_text': 'pre_cleaned_text_2'})\n",
        "\n",
        "similar_pairs_df = renamed_df_2.drop(columns=['review_id']) #drop unnecessary columns\n",
        "\n",
        "similar_pairs_df = similar_pairs_df.drop(columns=['user_id_1', 'user_id_2', 'business_id_1', 'business_id_2']) #drop unnecessary columns\n",
        "\n",
        "similar_pairs_df = similar_pairs_df[['review_id_1', #rearrange columns\n",
        "                                     'stars_1', \n",
        "                                     'pre_cleaned_text_1', \n",
        "                                     'review_id_2', \n",
        "                                     'stars_2', \n",
        "                                     'pre_cleaned_text_2']] "
      ],
      "metadata": {
        "id": "vai-XGn_WjPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. File Export**"
      ],
      "metadata": {
        "id": "oAjj63eHWt3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_data = similar_pairs_df.to_csv(index=False) #convert df to csv\n",
        "\n",
        "csv_filename = 'similar_pairs.csv' \n",
        "with open(csv_filename, 'w') as csv_file: #save csv file\n",
        "    csv_file.write(csv_data)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(csv_filename) #download csv file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "KnQqgiQeW0D_",
        "outputId": "4eacc870-6e36-4544-d14e-da8a970c20e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8f1f7a29-b321-4c9f-92ac-8c3540da64da\", \"similar_pairs.csv\", 7160982)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}