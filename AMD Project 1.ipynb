{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHabgZAdLlTu"
      },
      "source": [
        "## **1. Setting up the API and dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "luZ-bTqZW0Qh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f153785d-4569-47db-8924-2bdb06eca525"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading yelp-dataset.zip to /content\n",
            "100% 4.07G/4.07G [00:55<00:00, 10.6MB/s]\n",
            "100% 4.07G/4.07G [00:55<00:00, 78.4MB/s]\n",
            "mkdir: cannot create directory ‘reviews’: File exists\n",
            "Archive:  yelp-dataset.zip\n",
            "  inflating: reviews/Dataset_User_Agreement.pdf  \n",
            "  inflating: reviews/yelp_academic_dataset_business.json  \n",
            "  inflating: reviews/yelp_academic_dataset_checkin.json  \n",
            "  inflating: reviews/yelp_academic_dataset_review.json  \n",
            "  inflating: reviews/yelp_academic_dataset_tip.json  \n",
            "  inflating: reviews/yelp_academic_dataset_user.json  \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME'] = \"XXXXX\"\n",
        "os.environ['KAGGLE_KEY'] = \"XXXXX\"\n",
        "!kaggle datasets download -d yelp-dataset/yelp-dataset\n",
        "!mkdir reviews #creating a new directory for the dataset\n",
        "!unzip yelp-dataset.zip -d reviews #unzipping the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boDwWSFDLx_6"
      },
      "source": [
        "## **2. Pre-Processing Methods**\n",
        "\n",
        "#### *2.1 Data Cleaning*\n",
        "*   Lowercase All Text\n",
        "*   Removing Punctuations\n",
        "*   Word Lemmatisation\n",
        "*   Removing Stop-words\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "oG5V-gIXHbfg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f575315d-f90d-4b24-df49-59d1b6af56b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasketch in /usr/local/lib/python3.10/dist-packages (1.5.9)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.10/dist-packages (from datasketch) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from datasketch) (1.10.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#!pip install datasketch\n",
        "#!pip install nltk\n",
        "\n",
        "import string\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datasketch import MinHash, MinHashLSH\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "def clean_data(text):\n",
        "    text = text.lower()\n",
        "    text = remove_punctuation(text)\n",
        "    split_text = text.split()\n",
        "    remove_and_lemma = [lemma.lemmatize(word) for word in split_text if word not in stop_words]\n",
        "    cleaned_text = ' '.join(remove_and_lemma)\n",
        "    return cleaned_text\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *2.2 K-Shingling*"
      ],
      "metadata": {
        "id": "x4BLgScXd4wr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def k_shingle(text):\n",
        "    words = text.split()\n",
        "    for i in range(len(words) - 1):\n",
        "        shingle = ' '.join([words[i], words[i+1]])\n",
        "        yield shingle"
      ],
      "metadata": {
        "id": "2BnVjGubd2Hm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBupAky0MTI-"
      },
      "source": [
        "## **3. Processing Reviews & Building MinHashLSH Index**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "TwYPeB57BnGQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a207cdb-e8cd-4f2c-db9c-8150486dc031"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rProcessing Chunks:   0%|          | 0/60 [00:00<?, ?chunk/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 5 Processed Reviews:\n",
            "Review 1:\n",
            "Review ID: KU_O5udG6zpxOg-VcAEodg\n",
            "User ID: mh_-eMZ6K5RLWhZyISBhwA\n",
            "Business ID: XQfwVwDr-v0ZS3_CbbE5Xw\n",
            "Stars: 3\n",
            "Pre-cleaned Text: If you decide to eat here, just be aware it is going to take about 2 hours from beginning to end. We have tried it multiple times, because I want to like it! I have been to it's other locations in NJ and never had a bad experience. \n",
            "\n",
            "The food is good, but it takes a very long time to come out. The waitstaff is very young, but usually pleasant. We have just had too many experiences where we spent way too long waiting. We usually opt for another diner or restaurant on the weekends, in order to be done quicker.\n",
            "Shingles: ['decide eat', 'eat aware', 'aware going', 'going take', 'take 2', '2 hour', 'hour beginning', 'beginning end', 'end tried', 'tried multiple', 'multiple time', 'time want', 'want like', 'like location', 'location nj', 'nj never', 'never bad', 'bad experience', 'experience food', 'food good', 'good take', 'take long', 'long time', 'time come', 'come waitstaff', 'waitstaff young', 'young usually', 'usually pleasant', 'pleasant many', 'many experience', 'experience spent', 'spent way', 'way long', 'long waiting', 'waiting usually', 'usually opt', 'opt another', 'another diner', 'diner restaurant', 'restaurant weekend', 'weekend order', 'order done', 'done quicker']\n",
            "\n",
            "Review 2:\n",
            "Review ID: BiTunyQ73aT9WBnpR9DZGw\n",
            "User ID: OyoGAe7OKpv6SyGZT5g77Q\n",
            "Business ID: 7ATYjTIgM3jUlt4UM3IypQ\n",
            "Stars: 5\n",
            "Pre-cleaned Text: I've taken a lot of spin classes over the years, and nothing compares to the classes at Body Cycle. From the nice, clean space and amazing bikes, to the welcoming and motivating instructors, every class is a top notch work out.\n",
            "\n",
            "For anyone who struggles to fit workouts in, the online scheduling system makes it easy to plan ahead (and there's no need to line up way in advanced like many gyms make you do).\n",
            "\n",
            "There is no way I can write this review without giving Russell, the owner of Body Cycle, a shout out. Russell's passion for fitness and cycling is so evident, as is his desire for all of his clients to succeed. He is always dropping in to classes to check in/provide encouragement, and is open to ideas and recommendations from anyone. Russell always wears a smile on his face, even when he's kicking your butt in class!\n",
            "Shingles: ['ive taken', 'taken lot', 'lot spin', 'spin class', 'class year', 'year nothing', 'nothing compare', 'compare class', 'class body', 'body cycle', 'cycle nice', 'nice clean', 'clean space', 'space amazing', 'amazing bike', 'bike welcoming', 'welcoming motivating', 'motivating instructor', 'instructor every', 'every class', 'class top', 'top notch', 'notch work', 'work anyone', 'anyone struggle', 'struggle fit', 'fit workout', 'workout online', 'online scheduling', 'scheduling system', 'system make', 'make easy', 'easy plan', 'plan ahead', 'ahead there', 'there need', 'need line', 'line way', 'way advanced', 'advanced like', 'like many', 'many gym', 'gym make', 'make way', 'way write', 'write review', 'review without', 'without giving', 'giving russell', 'russell owner', 'owner body', 'body cycle', 'cycle shout', 'shout russell', 'russell passion', 'passion fitness', 'fitness cycling', 'cycling evident', 'evident desire', 'desire client', 'client succeed', 'succeed always', 'always dropping', 'dropping class', 'class check', 'check inprovide', 'inprovide encouragement', 'encouragement open', 'open idea', 'idea recommendation', 'recommendation anyone', 'anyone russell', 'russell always', 'always wear', 'wear smile', 'smile face', 'face even', 'even he', 'he kicking', 'kicking butt', 'butt class']\n",
            "\n",
            "Review 3:\n",
            "Review ID: saUsX_uimxRlCVr67Z4Jig\n",
            "User ID: 8g_iMtfSiwikVnbP2etR0A\n",
            "Business ID: YjUWPpI6HXG530lwP-fb2A\n",
            "Stars: 3\n",
            "Pre-cleaned Text: Family diner. Had the buffet. Eclectic assortment: a large chicken leg, fried jalapeño, tamale, two rolled grape leaves, fresh melon. All good. Lots of Mexican choices there. Also has a menu with breakfast served all day long. Friendly, attentive staff. Good place for a casual relaxed meal with no expectations. Next to the Clarion Hotel.\n",
            "Shingles: ['family diner', 'diner buffet', 'buffet eclectic', 'eclectic assortment', 'assortment large', 'large chicken', 'chicken leg', 'leg fried', 'fried jalapeño', 'jalapeño tamale', 'tamale two', 'two rolled', 'rolled grape', 'grape leaf', 'leaf fresh', 'fresh melon', 'melon good', 'good lot', 'lot mexican', 'mexican choice', 'choice also', 'also menu', 'menu breakfast', 'breakfast served', 'served day', 'day long', 'long friendly', 'friendly attentive', 'attentive staff', 'staff good', 'good place', 'place casual', 'casual relaxed', 'relaxed meal', 'meal expectation', 'expectation next', 'next clarion', 'clarion hotel']\n",
            "\n",
            "Review 4:\n",
            "Review ID: AqPFMleE6RsU23_auESxiA\n",
            "User ID: _7bHUi9Uuf5__HHc_Q8guQ\n",
            "Business ID: kxX2SOes4o-D3ZQBkiMRfA\n",
            "Stars: 5\n",
            "Pre-cleaned Text: Wow!  Yummy, different,  delicious.   Our favorite is the lamb curry and korma.  With 10 different kinds of naan!!!  Don't let the outside deter you (because we almost changed our minds)...go in and try something new!   You'll be glad you did!\n",
            "Shingles: ['wow yummy', 'yummy different', 'different delicious', 'delicious favorite', 'favorite lamb', 'lamb curry', 'curry korma', 'korma 10', '10 different', 'different kind', 'kind naan', 'naan dont', 'dont let', 'let outside', 'outside deter', 'deter almost', 'almost changed', 'changed mindsgo', 'mindsgo try', 'try something', 'something new', 'new youll', 'youll glad']\n",
            "\n",
            "Review 5:\n",
            "Review ID: Sx8TMOWLNuJBWer-0pcmoA\n",
            "User ID: bcjbaE6dDog4jkNY91ncLQ\n",
            "Business ID: e4Vwtrqf-wpJfwesgvdgxQ\n",
            "Stars: 4\n",
            "Pre-cleaned Text: Cute interior and owner (?) gave us tour of upcoming patio/rooftop area which will be great on beautiful days like today. Cheese curds were very good and very filling. Really like that sandwiches come w salad, esp after eating too many curds! Had the onion, gruyere, tomato sandwich. Wasn't too much cheese which I liked. Needed something else...pepper jelly maybe. Would like to see more menu options added such as salads w fun cheeses. Lots of beer and wine as well as limited cocktails. Next time I will try one of the draft wines.\n",
            "Shingles: ['cute interior', 'interior owner', 'owner gave', 'gave u', 'u tour', 'tour upcoming', 'upcoming patiorooftop', 'patiorooftop area', 'area great', 'great beautiful', 'beautiful day', 'day like', 'like today', 'today cheese', 'cheese curd', 'curd good', 'good filling', 'filling really', 'really like', 'like sandwich', 'sandwich come', 'come w', 'w salad', 'salad esp', 'esp eating', 'eating many', 'many curd', 'curd onion', 'onion gruyere', 'gruyere tomato', 'tomato sandwich', 'sandwich wasnt', 'wasnt much', 'much cheese', 'cheese liked', 'liked needed', 'needed something', 'something elsepepper', 'elsepepper jelly', 'jelly maybe', 'maybe would', 'would like', 'like see', 'see menu', 'menu option', 'option added', 'added salad', 'salad w', 'w fun', 'fun cheese', 'cheese lot', 'lot beer', 'beer wine', 'wine well', 'well limited', 'limited cocktail', 'cocktail next', 'next time', 'time try', 'try one', 'one draft', 'draft wine']\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Chunks: 100%|██████████| 60/60 [30:12<00:00, 30.22s/chunk]\n"
          ]
        }
      ],
      "source": [
        "chunk_size = 10000\n",
        "num_perm = 128 \n",
        "threshold = 0.75\n",
        "\n",
        "lsh = MinHashLSH(num_perm=num_perm, threshold=threshold) #initialise lsh\n",
        "\n",
        "processed_reviews = []\n",
        "num_reviews_processed = 0\n",
        "num_chunks = 60\n",
        "\n",
        "with tqdm(total=num_chunks, desc='Processing Chunks', unit='chunk') as pbar: #creating progress bar\n",
        "    # Iterate over the data chunks\n",
        "    for chunk_no, chunk in enumerate(pd.read_json(\"reviews/yelp_academic_dataset_review.json\", lines=True, chunksize=chunk_size)):\n",
        "        if chunk_no >= num_chunks:  #break the loop after desired number of chunks\n",
        "            break\n",
        "\n",
        "        chunk['pre_cleaned_text'] = chunk['text']  #add pre cleaned text column\n",
        "        chunk['text'] = chunk['text'].apply(clean_data)\n",
        "\n",
        "        processed_chunk = []\n",
        "        for ind, row in chunk.iterrows():  \n",
        "            shingles = list(k_shingle(row['text'])) #k-shingling\n",
        "            minhash = MinHash(num_perm=num_perm) \n",
        "            for shingle in shingles:\n",
        "                minhash.update(shingle.encode('utf-8'))\n",
        "            processed_chunk.append({\n",
        "                'review_id': row['review_id'],\n",
        "                'user_id': row['user_id'],\n",
        "                'business_id': row['business_id'],\n",
        "                'stars': row['stars'],\n",
        "                'pre_cleaned_text': row['pre_cleaned_text'],\n",
        "                'shingles': shingles,\n",
        "                'minhash': minhash\n",
        "            })\n",
        "\n",
        "        if chunk_no < 1:\n",
        "            print(\"First 5 Processed Reviews:\") #printing first 5 processed reviews\n",
        "            for i in range(5):\n",
        "                print(f\"Review {i+1}:\")\n",
        "                review = processed_chunk[i]\n",
        "                print(f\"Review ID: {review['review_id']}\")\n",
        "                print(f\"User ID: {review['user_id']}\")\n",
        "                print(f\"Business ID: {review['business_id']}\")\n",
        "                print(f\"Stars: {review['stars']}\")\n",
        "                print(f\"Pre-cleaned Text: {review['pre_cleaned_text']}\")\n",
        "                print(f\"Shingles: {review['shingles']}\")\n",
        "                print()\n",
        "\n",
        "        for item in processed_chunk:\n",
        "            lsh.insert(item['review_id'], item['minhash']) #index minhash objects in lsh\n",
        "        \n",
        "        processed_reviews.extend(processed_chunk)\n",
        "        pbar.update(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaoNRvmxFpih"
      },
      "source": [
        "\n",
        "## **4. Finding Similar Pairs**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "similar_pairs = []\n",
        "for chunk_no, chunk in enumerate(pd.read_json(\"reviews/yelp_academic_dataset_review.json\", lines=True, chunksize=chunk_size)):\n",
        "    if chunk_no >= num_chunks:  #break the loop after the specified number of chunks\n",
        "        break\n",
        "\n",
        "    chunk['pre_cleaned_text'] = chunk['text']  #add pre clean text column\n",
        "    chunk['text'] = chunk['text'].apply(clean_data)\n",
        "\n",
        "    for ind, row in chunk.iterrows():  \n",
        "        shingles = list(k_shingle(row['text'])) #k-shingling\n",
        "        minhash = MinHash(num_perm=num_perm)\n",
        "        for shingle in shingles:\n",
        "            minhash.update(shingle.encode('utf-8'))\n",
        "\n",
        "        similar_items = lsh.query(minhash) #lsh query for similar items\n",
        "\n",
        "        \n",
        "        for item in similar_items: \n",
        "            if item != row['review_id']:  #remove self-matches\n",
        "                similar_pairs.append((row['review_id'], item)) #appending similar pairs"
      ],
      "metadata": {
        "id": "Oa60RthKvjYX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Similar Pairs Dataframe**"
      ],
      "metadata": {
        "id": "PzMLfHTmWkFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_clean = pd.DataFrame(processed_reviews) \n",
        "\n",
        "similar_pairs_df = pd.DataFrame(similar_pairs, columns=['review_id_1', 'review_id_2']) #create df with similar pairs\n",
        "\n",
        "merged_df_1 = pd.merge(similar_pairs_df, data_clean[['review_id', #merge with original dataset using left join\n",
        "                                                     'user_id', \n",
        "                                                     'business_id', \n",
        "                                                     'stars', \n",
        "                                                     'pre_cleaned_text']],\n",
        "                       left_on='review_id_1', right_on='review_id', how='left')\n",
        "\n",
        "renamed_df_1 = merged_df_1.rename(columns={'user_id': 'user_id_1', #rename for clarity\n",
        "                                           'business_id': 'business_id_1',\n",
        "                                           'stars': 'stars_1',\n",
        "                                           'pre_cleaned_text': 'pre_cleaned_text_1'})\n",
        "\n",
        "dropped_df_1 = renamed_df_1.drop(columns=['review_id']) #drop unnecessary columns\n",
        "\n",
        "merged_df_2 = pd.merge(dropped_df_1, data_clean[['review_id', #merge using second review id using left join\n",
        "                                                 'user_id', \n",
        "                                                 'business_id', \n",
        "                                                 'stars', \n",
        "                                                 'pre_cleaned_text']],\n",
        "                       left_on='review_id_2', right_on='review_id', how='left')\n",
        "\n",
        "renamed_df_2 = merged_df_2.rename(columns={'user_id': 'user_id_2',\n",
        "                                           'business_id': 'business_id_2',\n",
        "                                           'stars': 'stars_2',\n",
        "                                           'pre_cleaned_text': 'pre_cleaned_text_2'})\n",
        "\n",
        "similar_pairs_df = renamed_df_2.drop(columns=['review_id'])\n",
        "\n",
        "similar_pairs_df = similar_pairs_df.drop(columns=['user_id_1', 'user_id_2', 'business_id_1', 'business_id_2']) #drop unnecessary columns\n",
        "\n",
        "similar_pairs_df = similar_pairs_df[['review_id_1', 'stars_1', 'pre_cleaned_text_1', 'review_id_2', 'stars_2', 'pre_cleaned_text_2']] #rearrange columns"
      ],
      "metadata": {
        "id": "vai-XGn_WjPn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. File Export**"
      ],
      "metadata": {
        "id": "oAjj63eHWt3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "csv_data = similar_pairs_df.to_csv(index=False) #convert df to csv\n",
        "\n",
        "csv_filename = 'similar_pairs.csv' \n",
        "with open(csv_filename, 'w') as csv_file: #save csv file\n",
        "    csv_file.write(csv_data)\n",
        "\n",
        "from google.colab import files\n",
        "files.download(csv_filename) #download csv file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "KnQqgiQeW0D_",
        "outputId": "4eacc870-6e36-4544-d14e-da8a970c20e2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8f1f7a29-b321-4c9f-92ac-8c3540da64da\", \"similar_pairs.csv\", 7160982)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}